{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from flair.data import Sentence\n",
    "from segtok.segmenter import split_single\n",
    "import random\n",
    "from flair.models import SequenceTagger\n",
    "import xml.etree.ElementTree as ET\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from pycorenlp import StanfordCoreNLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello all', 'my name is titipat the best lol player']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "properties={\n",
    "  'annotators': 'ssplit',\n",
    "  'outputFormat': 'json'\n",
    "  }\n",
    "def sentence_split(text, properties={'annotators': 'ssplit', 'outputFormat': 'json'}):\n",
    "    \"\"\"Split sentence using Stanford NLP\"\"\"\n",
    "    annotated = nlp.annotate(text, properties)\n",
    "    sentence_split = list()\n",
    "    for sentence in annotated['sentences']:\n",
    "        s = [t['word'] for t in sentence['tokens']]\n",
    "        k = [item.lower() for item in s if item not in [\",\", \".\", '...', '..']]\n",
    "        sentence_split.append(\" \".join(k))\n",
    "    return sentence_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader :\n",
    "    def __init__(self, _paths, _hint):\n",
    "        self.paths = _paths\n",
    "        self.files = []\n",
    "        self.inputs = []\n",
    "        self.data = []\n",
    "        self.train = []\n",
    "        self.valid = []\n",
    "        self.test = []\n",
    "        self.hintFile = _hint\n",
    "        self.splitTest = []\n",
    "        self.splitValid = []\n",
    "        self.splitTrain = []\n",
    "        self.postags = []\n",
    "        self.tagger = SequenceTagger.load('pos')\n",
    "\n",
    "    def listFiles(self):\n",
    "        for path in self.paths:\n",
    "            self.files = self.files + glob.glob(path + \"*.sgm\")\n",
    "        print('Loaded {} files'.format(len(self.files)), end='\\n')\n",
    "\n",
    "    def splitHint(self):\n",
    "        train = open(self.hintFile + \"/train.txt\", \"r+\")\n",
    "        valid = open(self.hintFile + \"/valid.txt\", \"r+\")\n",
    "        test = open(self.hintFile + \"/test.txt\", \"r+\")\n",
    "\n",
    "        text_train = train.read()\n",
    "        text_valid = valid.read()\n",
    "        text_test = test.read()\n",
    "\n",
    "        list_train = text_train.split(\"\\n\")\n",
    "        for line in list_train:\n",
    "            self.splitTrain.append(line + \".sgm\")\n",
    "\n",
    "        list_valid = text_valid.split(\"\\n\")\n",
    "        for line in list_valid:\n",
    "            self.splitValid.append(line + \".sgm\")\n",
    "\n",
    "        list_test = text_test.split(\"\\n\")\n",
    "        for line in list_test:\n",
    "            self.splitTest.append(line + \".sgm\")\n",
    "\n",
    "    def readData(self):\n",
    "        print('Starting to Prepare {} files'.format(len(self.files)), end='\\n')\n",
    "        print(\" \")\n",
    "        iteration = 0\n",
    "        for item in self.files:\n",
    "            file1 = open(item, \"r+\")\n",
    "            text = file1.read()\n",
    "            sample = {\"text\": text, \"id\": item, \"parse\": BeautifulSoup(text, 'lxml')}\n",
    "            sample['file_name'] = sample['id'].split(\"/\")[-1]\n",
    "            if sample['file_name'] in self.splitTrain:\n",
    "                sample['type'] = \"train\"\n",
    "            elif sample['file_name'] in self.splitValid:\n",
    "                sample['type'] = \"valid\"\n",
    "            elif sample['file_name'] in self.splitTest:\n",
    "                sample['type'] = \"test\"\n",
    "            else:\n",
    "                sample['type'] = \"ignore\"\n",
    "            text = sample['parse'].select_one(\"TEXT\").prettify()\n",
    "            text = re.sub('<[^<]+>', \"\\n\",text)\n",
    "            text = text.replace(\"Dr.\", \"Dr \").replace(\"Mr.\", \"Mr \").replace(\"Al-\", \"Al\").replace(\"/\", \" \").replace(\"'s\", \" s\").replace(\".-\", \"-\").replace(\"-\", \" \").replace(\"(\", \" \").replace(\")\", \" \").replace(\"%\", ' percent ')\n",
    "            x = re.findall(r'[a-z]\\.[A-Z]', text)\n",
    "            for item in x:\n",
    "                item = item.replace(\".\", \"\\.\")\n",
    "                y = item.replace(\"\\.\", \" . \")\n",
    "                text = re.sub(item, y, text)\n",
    "#             x = re.findall(r'[a-z]-[a-zA-Z]', text)\n",
    "#             for item in x:\n",
    "#                 y = item.replace(\"-\", \" \")\n",
    "#                 text = re.sub(item, y, text)\n",
    "            text = re.sub(' +', ' ', text)\n",
    "            sample['parse'] = text\n",
    "            splitted = sentence_split(text)\n",
    "#             splitted = text.split(\".\")\n",
    "#             splitted = [re.sub(' +', ' ', re.sub(r'[^\\w\\s]',' ',data)).replace(\" t \", \"'t \") for data in splitted if len(data) > 2]\n",
    "            splitted = [re.sub(' +', ' ', data).replace(\"'s\", \"' s\").replace(\"' t \", \"'t \") for data in splitted if len(data) >= 1]\n",
    "            sentences = [Sentence(re.sub(r'[^\\w\\s]',' ',sent)) for sent in splitted if sent]\n",
    "            sentences = [sentence for sentence in sentences if len(sentence) >= 1]\n",
    "            sample['sentences'] = sentences\n",
    "            self.inputs.append(sample)\n",
    "            file1.close()\n",
    "            iteration += 1\n",
    "            print('Prepared {} files'.format(iteration), end='\\r')\n",
    "        print('Preparation resulted in {} inputs'.format(len(self.inputs)))\n",
    "\n",
    "    def prepareLables(self):\n",
    "        print(\"starting to prepare lables\", end='\\n')\n",
    "        print(\" \")\n",
    "        iteration = 0\n",
    "        for sample in self.inputs:\n",
    "            id = sample['id'].split('.sgm')[0]\n",
    "            path = id + \".apf.xml\"\n",
    "            sample['annotation_path'] = path\n",
    "            iteration += 1\n",
    "            print('Prepared {} Lables'.format(iteration), end='\\r')\n",
    "\n",
    "    def Sort(self, sub_li): \n",
    "  \n",
    "        # reverse = None (Sorts in Ascending order) \n",
    "        # key is set to sort using second element of  \n",
    "        # sublist lambda has been used \n",
    "        sub_li.sort(key = lambda x: x[3]) \n",
    "        return sub_li \n",
    "    \n",
    "    def readLables(self):\n",
    "        print(\"starting to read lables\", end='\\n')\n",
    "        iteration = 0\n",
    "        total_ann = 0\n",
    "        for sample in self.inputs:\n",
    "            tree = ET.parse(sample['annotation_path']) \n",
    "            root = tree.getroot() \n",
    "            entities = root.findall(\".//entity\")\n",
    "            _all = []\n",
    "            for entity in entities:\n",
    "                mentions = entity.findall(\".//entity_mention\")\n",
    "                _all.extend([[mention.find(\"./head/charseq\").text, mention.find(\"./head/charseq\").attrib['START'], entity.attrib, mention.attrib['ID'], mention.find(\"./head/charseq\").attrib['END'], mention.find(\"./extent/charseq\").text] for mention in mentions])\n",
    "            _all = sorted(_all, key=lambda x:int(x[1]))\n",
    "            entities = [[x[0],x[2]['TYPE'],x[2]['SUBTYPE'],int(x[1]),x[3], int(x[4]), x[5]] for x in _all]\n",
    "            sample['annotated'] = self.Sort(entities)\n",
    "            relations = root.findall(\".//relation\")\n",
    "            _all = []\n",
    "            for relation in relations:\n",
    "                mentions = relation.findall(\".//relation_mention\")\n",
    "                _all.extend([[mention.findall(\"./relation_mention_argument\")[0].attrib['REFID'], mention.findall(\"./relation_mention_argument\")[1].attrib['REFID'], relation.attrib, mention.attrib[\"ID\"]] for mention in mentions])\n",
    "            sample['relations'] = [(x[0],\n",
    "                                    x[1],\n",
    "                                    x[2]['TYPE'],\n",
    "                                    x[2]['SUBTYPE'],\n",
    "                                    x[3]\n",
    "                                   )\n",
    "                                       for x in _all if x[2]['TYPE'] != \"METONYMY\"]\n",
    "            sample['relations'].extend([(x[0],\n",
    "                                        x[1],\n",
    "                                        x[2]['TYPE'],\n",
    "                                        \"UNKNOWN\",\n",
    "                                        x[3]\n",
    "                                       )\n",
    "                                       for x in _all if x[2]['TYPE'] == \"METONYMY\"])\n",
    "#             sample['relations'] = _all\n",
    "            iteration += 1\n",
    "            total_ann += len(sample['annotated'])\n",
    "            print('Read {} inputs with total {} annotations'.format(iteration, total_ann), end='\\r')\n",
    "\n",
    "    def match(self, s1, s2):\n",
    "        if s1 == s2:\n",
    "            return True\n",
    "        elif s1 in s2.split(\"-\"):\n",
    "            return True\n",
    "        else:\n",
    "            if s1 == \"u.s\":\n",
    "                s1 = \"u.s.\"\n",
    "            if s1 == \"u.k\":\n",
    "                s1 = \"u.k.\"\n",
    "            if s1 == \"u.n\":\n",
    "                s1 = \"u.n.\"\n",
    "            if s1 == \"dr\":\n",
    "                s1 = \"dr.\"\n",
    "            if s1 == \"mr\":\n",
    "                s1 = \"mr.\"\n",
    "            if s1 == \"l\":\n",
    "                s1 == \"led\"\n",
    "            if s1 == s2:\n",
    "                return True\n",
    "            elif s1 in s2.split(\"-\"):\n",
    "                return True\n",
    "            if s2.count(s1):\n",
    "                if len(s2) >= 3 and (len(s2) <= len(s1) + 1 or len(s1) / len(s2) >= 0.65):\n",
    "                    return True\n",
    "                if len(s1) >= 5 and len(s2) >= 8:\n",
    "                    return True\n",
    "                if len(s1) <= 4 and s1 == s2[0:len(s1)] or s1 == s2[len(s2)-len(s1): ]:\n",
    "                    return True\n",
    "#                 if s1 == \"asia\" and s2 == \"asiaagent\":\n",
    "#                     return True\n",
    "                    \n",
    "            s1 = re.sub(r'[^\\w\\s]','',s1)\n",
    "            s2 = re.sub(r'[^\\w\\s]','',s2)\n",
    "            if s1 == s2:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "        \n",
    "    def lableInputs(self):\n",
    "        print(\"starting to lable inputs\", end='\\n')\n",
    "        print(\" \")\n",
    "        iteration = 0\n",
    "        self.total_passed = 0\n",
    "        self.not_checked = 0\n",
    "        self.not_checked_data = []\n",
    "        for sample in self.inputs:\n",
    "            if sample['type'] == \"ignore\":\n",
    "                continue\n",
    "            total_phrases = 0\n",
    "            passed = 0\n",
    "            sample['inputs'] = []\n",
    "            sample['phrases'] = []\n",
    "            # words array cotains arrays of all sentences inside a file\n",
    "            checked = 0\n",
    "            for sentence in sample['sentences']:\n",
    "                annotated_sentence = []\n",
    "                check = False\n",
    "                #                 print(sentence)\n",
    "                sentence1 = sentence\n",
    "                sentence = sentence.to_tokenized_string().split(\" \")\n",
    "                phrases = []\n",
    "                start = 0\n",
    "                for iteration in range(len(sample['annotated'])):\n",
    "                    if iteration < checked:\n",
    "                        continue\n",
    "                    annotation = sample['annotated'][iteration]\n",
    "                    if iteration >= 1:\n",
    "                        if annotation[3] <= sample['annotated'][iteration-1][5]:\n",
    "                            passed += 1\n",
    "                            if sample['type'] == \"test\":\n",
    "                                self.total_passed += 1\n",
    "                            checked += 1\n",
    "                            continue\n",
    "#                             start = phrases[-1]['before']\n",
    "                    _list = []\n",
    "                    text = annotation[0]\n",
    "                    x = re.findall(r'[a-z]\\.[A-Z]', text)\n",
    "                    for item in x:\n",
    "                        item = item.replace(\".\", \"\\.\")\n",
    "                        y = item.replace(\"\\.\", \" . \")\n",
    "                        text = re.sub(item, y, text)\n",
    "#                     x = re.findall(r'[a-z]-[A-Z]', text)\n",
    "#                     for item in x:\n",
    "#                         y = item.replace(\"-\", \" \")\n",
    "#                         text = re.sub(item, y, text)\n",
    "                    text = text.replace(\"Al-\", \"Al\").replace(\"Dr.\", \"Dr \").replace(\"Mr.\", \"Mr \").replace(\"/\", \" \").replace(\"'s\", \" s\").replace(\".-\", \"-\").replace(\"-\", \" \").replace(\"%\", ' percent ')\n",
    "                    text = re.sub(' +', ' ', text)\n",
    "                    try:\n",
    "                        labels = sentence_split(re.sub(r'[^\\w\\s]',' ',text))[0]\n",
    "                    except:\n",
    "                        print(\"file name is \", sample['annotation_path'])\n",
    "                        print(\"text is \", text)\n",
    "                        print(\"iteration is \", iteration)\n",
    "                        raise\n",
    "                    labels = Sentence(labels).to_tokenized_string().replace(\"\\n\", \" \").split(\" \")\n",
    "#                     if labels[0] == \"u.s\":\n",
    "#                         labels[0] = \"u.s.\"\n",
    "#                     if labels[0] == \"u.k\":\n",
    "#                         labels[0] = \"u.k.\"\n",
    "#                     if labels[0] == \"u.n\":\n",
    "#                         labels[0] = \"u.n.\"\n",
    "                    exist_check = False\n",
    "                    for _itt in range(start, len(sentence)):\n",
    "                        if self.match(labels[0], sentence[_itt]):\n",
    "                            exist_check = True\n",
    "                            value_index = _itt - start\n",
    "                            if len(labels) > len(sentence[_itt:]):\n",
    "                                exist_check = False\n",
    "                            else:\n",
    "                                for _it in range(len(labels)):\n",
    "                                    if self.match(labels[_it], sentence[start+value_index+_it]):\n",
    "                                        _list.append(sentence[start+value_index+_it])\n",
    "                                    else:\n",
    "                                        break\n",
    "                                if len(_list) != len(labels):\n",
    "                                    _list = []\n",
    "                                    exist_check = False\n",
    "                        if exist_check:\n",
    "                            break\n",
    "                    if not exist_check:\n",
    "                        break\n",
    "                        \n",
    "                    check = True\n",
    "                    example = {\"text\": annotation[0], \"label\": annotation[1], 'list': _list, 'value_index': value_index, 'before': start, 'id': annotation[4], 'start': annotation[3], 'end' : annotation[5]}\n",
    "\n",
    "                    for _it in range(start, start+value_index):\n",
    "                        _input = {\"word\": sentence[_it], 'w-type': '-O-', 'type': '-O-', 'subtype': '-O-',\n",
    "                                  'set': sample['type'], 'index': self.index_finder(\"-O-\")}\n",
    "                        annotated_sentence.append(_input)\n",
    "                    if len(_list) == 1:\n",
    "                        _input = {\"word\": _list[0], 'w-type': annotation[1], 'type': \"L-\" + annotation[1], 'subtype': annotation[2],\n",
    "                                          'set': sample['type'], 'start': annotation[3], 'index': self.index_finder(annotation[1])}\n",
    "                        annotated_sentence.append(_input)\n",
    "                    elif len(_list) == 2:\n",
    "                        _input = {\"word\": _list[0], 'w-type': annotation[1], 'type': \"B-\" + annotation[1], 'subtype': annotation[2],\n",
    "                                  'set': sample['type'], 'start': annotation[3], 'index': self.index_finder(annotation[1])}\n",
    "                        annotated_sentence.append(_input)\n",
    "                        _input = {\"word\": _list[1], 'w-type': annotation[1], 'type': \"L-\" + annotation[1], 'subtype': annotation[2],\n",
    "                                  'set': sample['type'], 'start': annotation[3], 'index': self.index_finder(annotation[1])}\n",
    "                        annotated_sentence.append(_input)\n",
    "                    else:\n",
    "                        _input = {\"word\": _list[0], 'w-type': annotation[1], 'type': \"B-\" + annotation[1], 'subtype': annotation[2],\n",
    "                                  'set': sample['type'], 'start': annotation[3], 'index': self.index_finder(annotation[1])}\n",
    "                        annotated_sentence.append(_input)\n",
    "                        _input = {\"word\": _list[-1], 'w-type': annotation[1], 'type': \"L-\" + annotation[1], 'subtype': annotation[2],\n",
    "                                  'set': sample['type'], 'start': annotation[3], 'index': self.index_finder(annotation[1])}\n",
    "                        annotated_sentence.append(_input)\n",
    "                        for item in range(1, len(_list) - 1):\n",
    "                            _input = {\"word\": _list[item], 'w-type': annotation[1], 'type': \"I-\" + annotation[1], 'subtype': annotation[2],\n",
    "                                      'set': sample['type'], 'start': annotation[3], 'index': self.index_finder(annotation[1])}\n",
    "                            annotated_sentence.append(_input)\n",
    "                    start = start + value_index + len(_list)\n",
    "                    example['after'] = start\n",
    "                    example['boundary'] = [example['before']+example['value_index'], example['after'] - 1]\n",
    "                    checked = iteration+1\n",
    "                    phrases.append(example)\n",
    "                for _it in range(start, len(sentence)):\n",
    "                        _input = {\"word\": sentence[_it], 'w-type': '-O-', 'type': '-O-', 'subtype': '-O-',\n",
    "                                  'set': sample['type'], 'index': self.index_finder('-O-')}\n",
    "                        annotated_sentence.append(_input)\n",
    "                # annotated_sentence is an array of annotated words inside a sentence\n",
    "                relations = []\n",
    "                all_relations = []\n",
    "                phrases_ids = [phrase['id'] for phrase in phrases]\n",
    "                for phrase in phrases:\n",
    "                    for rel in sample['relations']:\n",
    "                        if rel in relations:\n",
    "                            continue\n",
    "                        if rel[0] == phrase['id'] and rel[1] in phrases_ids:\n",
    "                            relations.append(rel)\n",
    "                        elif (rel[1] not in phrases_ids or rel[0] not in phrases_ids) and (rel[0] == phrase['id'] or rel[1] == phrase['id']):\n",
    "                            all_relations.append(rel)\n",
    "#                 if(len(all_relations) != len(relations)):\n",
    "#                     print(\"relation not match \", sample['annotation_path'])\n",
    "                        \n",
    "                sample['inputs'].append(annotated_sentence)\n",
    "                sample['phrases'].append(phrases)\n",
    "                total_phrases += len(phrases)\n",
    "                if check:\n",
    "                    self.data.append({\"words\": annotated_sentence, \"sentence\": sentence1.to_tokenized_string(), \"phrases\": phrases, 'relations': relations, 'all_relations': all_relations, 'file': sample['annotation_path']})\n",
    "                elif sample['type'] == \"test\":\n",
    "                    self.not_checked_data.append({\"words\": annotated_sentence, \"sentence\": sentence1.to_tokenized_string(), \"phrases\": phrases, 'relations': relations, 'all_relations': all_relations, 'file': sample['annotation_path']})\n",
    "                    self.not_checked += 1\n",
    "                    self.data.append({\"words\": annotated_sentence, \"sentence\": sentence1.to_tokenized_string(), \"phrases\": phrases, 'relations': relations, 'all_relations': all_relations, 'file': sample['annotation_path']})\n",
    "            iteration += 1\n",
    "            if(total_phrases + passed != len(sample['annotated'])):\n",
    "                print(\"phrase not match \", sample['annotation_path'])\n",
    "            print('Labled {} inputs with total number of sentences with entities of {} sentences'.format(iteration, len(\n",
    "                self.data)), end='\\r')\n",
    "        self.data = [item for item in self.data if len(item[\"words\"]) != 0]\n",
    "        \n",
    "    def index_finder(self, _class, choices=[\"ORG\", \"FAC\", \"PER\", \"VEH\", \"LOC\", \"WEA\", \"GPE\", \"-O-\"]):\n",
    "        return choices.index(_class)\n",
    "\n",
    "    def posTagFinder(self):\n",
    "        _it = 0\n",
    "        for _it in range(1500):\n",
    "            item = random.choice(self.data)\n",
    "            temp = Sentence(item['sentence'])\n",
    "            self.tagger.predict(temp)\n",
    "            _dict = temp.to_dict(tag_type='pos')\n",
    "            self.postags.extend([sample['type'] for sample in _dict['entities'] if sample['type'] not in self.postags])\n",
    "        self.postags = list(set(self.postags))\n",
    "    \n",
    "    def fire(self):\n",
    "        self.listFiles()\n",
    "        self.splitHint()\n",
    "        self.readData()\n",
    "        self.prepareLables()\n",
    "        self.readLables()\n",
    "        self.lableInputs()\n",
    "#         self.posTagFinder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-09 19:40:12,056 loading file /home/hfaghihi/.flair/models/en-pos-ontonotes-v0.4.pt\n",
      "Loaded 860 files\n",
      "Starting to Prepare 860 files\n",
      " \n",
      "Preparation resulted in 860 inputs\n",
      "starting to prepare lables\n",
      " \n",
      "starting to read lables\n",
      "starting to lable inputsal 68609 annotations\n",
      " \n",
      "Labled 258 inputs with total number of sentences with entities of 15651 sentences\r"
     ]
    }
   ],
   "source": [
    "data_path = [\n",
    "     \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/bc/fp1/\",\n",
    "     \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/bc/fp2/\",\n",
    "     \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/bn/fp1/\",\n",
    "     \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/bn/fp2/\",\n",
    "#      \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/cts/fp1/\",\n",
    "#      \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/cts/fp2/\",\n",
    "     \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/nw/fp1/\",\n",
    "     \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/nw/fp2/\",\n",
    "#      \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/un/fp1/\",\n",
    "#      \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/un/fp2/\",\n",
    "     \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/wi/fp1/\",\n",
    "     \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/wi/fp2/\", \n",
    "]\n",
    "\n",
    "hint_path = \"/home/hfaghihi/LDC2006T06/split\"\n",
    "\n",
    "data = DataLoader(data_path, hint_path)\n",
    "data.fire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "281\n",
      "{'words': [{'word': 'they', 'w-type': '-O-', 'type': '-O-', 'subtype': '-O-', 'set': 'test', 'index': 7}, {'word': 'come', 'w-type': '-O-', 'type': '-O-', 'subtype': '-O-', 'set': 'test', 'index': 7}, {'word': 'in', 'w-type': '-O-', 'type': '-O-', 'subtype': '-O-', 'set': 'test', 'index': 7}, {'word': 'a', 'w-type': '-O-', 'type': '-O-', 'subtype': '-O-', 'set': 'test', 'index': 7}, {'word': 'bag', 'w-type': '-O-', 'type': '-O-', 'subtype': '-O-', 'set': 'test', 'index': 7}], 'sentence': 'they come in a bag', 'phrases': [], 'relations': [], 'all_relations': [], 'file': '/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/bc/fp1/CNN_IP_20030402.1600.02-2.apf.xml'}\n",
      "281\n"
     ]
    }
   ],
   "source": [
    "print(data.total_passed)\n",
    "print(data.not_checked)\n",
    "print(data.not_checked_data[1])\n",
    "number = 0\n",
    "for item in data.data:\n",
    "    if not len(item['phrases']):\n",
    "        number += 1\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data.inputs:\n",
    "    if item['annotation_path'] == \"/home/hfaghihi/LDC2006T06/ace_2005_td_v7/data/English/bc/fp1/CNN_IP_20030402.1600.02-2.apf.xml\":\n",
    "        sample = item\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['inputs'] = []\n",
    "sample['phrases'] = []\n",
    "total_phrases = 0\n",
    "# words array cotains arrays of all sentences inside a file\n",
    "checked = 0\n",
    "passed = 0\n",
    "passed_items = []\n",
    "before_passed = []\n",
    "for sentence in sample['sentences']:\n",
    "    annotated_sentence = []\n",
    "    check = False\n",
    "    #                 print(sentence)\n",
    "    sentence1 = sentence\n",
    "    sentence = sentence.to_tokenized_string().split(\" \")\n",
    "    print(\"sentence is \", sentence)\n",
    "    phrases = []\n",
    "    start = 0\n",
    "    count = 0\n",
    "    for iteration in range(len(sample['annotated'])):\n",
    "        count += 1\n",
    "        if iteration < checked:\n",
    "            continue\n",
    "        annotation = sample['annotated'][iteration]\n",
    "        if iteration >= 1:\n",
    "            if annotation[3] <= sample['annotated'][iteration-1][5]:\n",
    "                passed += 1\n",
    "                checked += 1\n",
    "                before_passed.append(sample['annotated'][iteration-1])\n",
    "                passed_items.append(annotation)\n",
    "                continue\n",
    "        _list = []\n",
    "        text = annotation[0]\n",
    "        x = re.findall(r'[a-z]\\.[A-Z]', text)\n",
    "        for item in x:\n",
    "            item = item.replace(\".\", \"\\.\")\n",
    "            y = item.replace(\"\\.\", \" . \")\n",
    "            text = re.sub(item, y, text)\n",
    "        x = re.findall(r'[a-z]-[A-Z]', text)\n",
    "        for item in x:\n",
    "            y = item.replace(\"-\", \" \")\n",
    "            text = re.sub(item, y, text)\n",
    "        text = text.replace(\"Al-\", \"Al\").replace(\"Dr.\", \"Dr \").replace(\"Mr.\", \"Mr \").replace(\"/\", \" \").replace(\"'s\", \" s\").replace(\".-\", \"-\").replace(\"-\", \" \").replace(\"%\", ' percent ')\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        labels = sentence_split(re.sub(r'[^\\w\\s]',' ',text))[0]\n",
    "        print(\"labels are \", labels)\n",
    "        labels = Sentence(labels).to_tokenized_string().replace(\"\\n\", \" \").split(\" \")\n",
    "        print(labels[0])\n",
    "\n",
    "#         if labels[0] == \"u.s\":\n",
    "#             labels[0] = \"u.s.\"\n",
    "#         if labels[0] == \"u.k\":\n",
    "#             labels[0] = \"u.k.\"\n",
    "#         if labels[0] == \"u.n\":\n",
    "#             labels[0] = \"u.n.\"\n",
    "        exist_check = False\n",
    "        for _itt in range(start, len(sentence)):\n",
    "            checking = False\n",
    "            if data.match(labels[0], sentence[_itt]):\n",
    "                exist_check = True\n",
    "#                             try:\n",
    "                value_index = _itt - start\n",
    "#                             except:\n",
    "#                                 print(\"error\")\n",
    "#                                 print(labels[0])\n",
    "#                                 print(sample['annotation_path'])\n",
    "#                                 print(sentence[start:])\n",
    "#                                 raise\n",
    "                if len(labels) > len(sentence[_itt:]):\n",
    "                    print(\"here surprise\")\n",
    "                    exist_check = False\n",
    "                else:\n",
    "                    for _it in range(len(labels)):\n",
    "                        if data.match(labels[_it], sentence[start+value_index+_it]):\n",
    "                            _list.append(sentence[start+value_index+_it])\n",
    "                        else:\n",
    "                            print(\"not matched \", labels[_it], \" and \", sentence[start+value_index+_it])\n",
    "                            break\n",
    "                    if len(_list) != len(labels):\n",
    "                        print(_list)\n",
    "                        print(labels)\n",
    "                        _list = []\n",
    "                        print(\"here 4\")\n",
    "                        exist_check = False\n",
    "            if exist_check:\n",
    "                break\n",
    "        if not exist_check:\n",
    "            print(\"here 3\")\n",
    "            break\n",
    "\n",
    "#                     if labels[0] in sentence[start:]:\n",
    "#                         value_index = sentence[start:].index(labels[0])\n",
    "#                         if len(labels) > len(sentence[value_index:]):\n",
    "#                             break\n",
    "#                     else:\n",
    "#                         break\n",
    "\n",
    "        \n",
    "        check = True\n",
    "#         print(\"here 6\")\n",
    "        example = {\"text\": annotation[0], \"label\": annotation[1], 'list': _list, 'value_index': value_index, 'before': start, 'id': annotation[4], 'start': annotation[3]}\n",
    "\n",
    "        for _it in range(start, start+value_index):\n",
    "            _input = {\"word\": sentence[_it], 'w-type': '-O-', 'type': '-O-', 'subtype': '-O-',\n",
    "                      'set': sample['type'], 'index': data.index_finder(\"-O-\")}\n",
    "            annotated_sentence.append(_input)\n",
    "        if len(_list) == 1:\n",
    "            _input = {\"word\": _list[0], 'w-type': annotation[1], 'type': \"L-\" + annotation[1], 'subtype': annotation[2],\n",
    "                              'set': sample['type'], 'start': annotation[3], 'index': data.index_finder(annotation[1])}\n",
    "            annotated_sentence.append(_input)\n",
    "        elif len(_list) == 2:\n",
    "            _input = {\"word\": _list[0], 'w-type': annotation[1], 'type': \"B-\" + annotation[1], 'subtype': annotation[2],\n",
    "                      'set': sample['type'], 'start': annotation[3], 'index': data.index_finder(annotation[1])}\n",
    "            annotated_sentence.append(_input)\n",
    "            _input = {\"word\": _list[1], 'w-type': annotation[1], 'type': \"L-\" + annotation[1], 'subtype': annotation[2],\n",
    "                      'set': sample['type'], 'start': annotation[3], 'index': data.index_finder(annotation[1])}\n",
    "            annotated_sentence.append(_input)\n",
    "        else:\n",
    "            _input = {\"word\": _list[0], 'w-type': annotation[1], 'type': \"B-\" + annotation[1], 'subtype': annotation[2],\n",
    "                      'set': sample['type'], 'start': annotation[3], 'index': data.index_finder(annotation[1])}\n",
    "            annotated_sentence.append(_input)\n",
    "            _input = {\"word\": _list[-1], 'w-type': annotation[1], 'type': \"L-\" + annotation[1], 'subtype': annotation[2],\n",
    "                      'set': sample['type'], 'start': annotation[3], 'index': data.index_finder(annotation[1])}\n",
    "            annotated_sentence.append(_input)\n",
    "            for item in range(1, len(_list) - 1):\n",
    "                _input = {\"word\": _list[item], 'w-type': annotation[1], 'type': \"I-\" + annotation[1], 'subtype': annotation[2],\n",
    "                          'set': sample['type'], 'start': annotation[3], 'index': data.index_finder(annotation[1])}\n",
    "                annotated_sentence.append(_input)\n",
    "        start = start + value_index + len(_list)\n",
    "        example['after'] = start\n",
    "        example['boundary'] = [example['before']+example['value_index'], example['after'] - 1]\n",
    "        checked = iteration+1\n",
    "        phrases.append(example)\n",
    "    print(phrases)\n",
    "    print(count)\n",
    "    for _it in range(start, len(sentence)):\n",
    "            _input = {\"word\": sentence[_it], 'w-type': '-O-', 'type': '-O-', 'subtype': '-O-',\n",
    "                      'set': sample['type'], 'index': data.index_finder('-O-')}\n",
    "            annotated_sentence.append(_input)\n",
    "    # annotated_sentence is an array of annotated words inside a sentence\n",
    "    relations = []\n",
    "    all_relations = []\n",
    "    phrases_ids = [phrase['id'] for phrase in phrases]\n",
    "    for phrase in phrases:\n",
    "        for rel in sample['relations']:\n",
    "            if rel[0] == phrase['id'] and rel[1] in phrases_ids:\n",
    "                relations.append(rel)\n",
    "            elif rel[0] == phrase['id'] or rel[1] == phrase['id']:\n",
    "                all_relations.append(rel)\n",
    "\n",
    "    sample['inputs'].append(annotated_sentence)\n",
    "    sample['phrases'].append(phrases)\n",
    "    total_phrases += len(phrases)\n",
    "print(total_phrases+passed)\n",
    "print(len(sample['annotated']))\n",
    "print(passed_items)\n",
    "print(before_passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15651\n",
      "iteration is  71\n",
      "CNN_IP_20030402.1600.02-2-E13-33\n",
      "it is in  72\n",
      "merging in process\n",
      "iteration is  156\n",
      "CNN_IP_20030412.1600.03-E2-148\n",
      "it is in  157\n",
      "merging in process\n",
      "iteration is  771\n",
      "CNN_LE_20030504.1200.02-1-E40-97\n",
      "CNN_LE_20030504.1200.02-1-E40-97\n",
      "so i would n t dwell on that point specifically because saddam hussein clearly was a producer and a user of biological and chemical weapons\n",
      "error\n",
      "iteration is  822\n",
      "CNN_IP_20030405.1600.02-E11-19\n",
      "it is in  823\n",
      "merging in process\n",
      "iteration is  914\n",
      "CNN_IP_20030403.1600.00-2-E16-106\n",
      "it is in  915\n",
      "merging in process\n",
      "iteration is  983\n",
      "CNN_CF_20030303.1900.00-E70-123\n",
      "it is in  984\n",
      "merging in process\n",
      "iteration is  1197\n",
      "CNN_IP_20030329.1600.01-1-E2-99\n",
      "CNN_IP_20030329.1600.01-1-E2-99\n",
      "judy wedeman ben wedeman describing coalition hits in the area where he is the town of kalak in northern iraq\n",
      "error\n",
      "iteration is  1544\n",
      "CNN_IP_20030328.1600.07-E35-68\n",
      "it is in  1545\n",
      "merging in process\n",
      "iteration is  1614\n",
      "CNN_IP_20030408.1600.04-E102-145\n",
      "it is in  1615\n",
      "merging in process\n",
      "iteration is  1668\n",
      "CNN_CF_20030305.1900.00-2-E15-47\n",
      "it is in  1669\n",
      "merging in process\n",
      "iteration is  1922\n",
      "CNN_IP_20030329.1600.00-4-E12-24\n",
      "it is in  1923\n",
      "merging in process\n",
      "iteration is  1959\n",
      "CNN_IP_20030402.1600.00-3-E27-75\n",
      "CNN_IP_20030402.1600.00-3-E27-75\n",
      "but they are doing a job you know i do n t know the thoughts of the soldiers the men and women that went into that building and brought her out\n",
      "error\n",
      "iteration is  2477\n",
      "CNN_IP_20030402.1600.02-2-E13-31\n",
      "it is in  2478\n",
      "merging in process\n",
      "iteration is  3304\n",
      "CNN_IP_20030403.1600.00-2-E15-107\n",
      "it is in  3305\n",
      "merging in process\n",
      "iteration is  3323\n",
      "CNN_LE_20030504.1200.01-E23-68\n",
      "CNN_LE_20030504.1200.01-E23-68\n",
      "blitzer all right tell me what rumsfeld i think what he wrote me about was the fact that the inter agency process where we have all these fbi and department of justice and cia and dia and what have you involved in interrogating these detainees it takes time to find out what intelligence they have\n",
      "error\n",
      "iteration is  3629\n",
      "CNN_CF_20030304.1900.06-2-E6-14\n",
      "it is in  3630\n",
      "merging in process\n",
      "iteration is  4054\n",
      "CNN_CF_20030305.1900.00-2-E19-44\n",
      "it is in  4055\n",
      "merging in process\n",
      "iteration is  4062\n",
      "CNN_IP_20030410.1600.03-2-E1-2\n",
      "it is in  4063\n",
      "merging in process\n",
      "iteration is  4318\n",
      "CNN_IP_20030329.1600.00-4-E9-24\n",
      "it is in  4319\n",
      "merging in process\n",
      "iteration is  4832\n",
      "CNN_ENG_20030624_153103.16-E30-51\n",
      "it is in  4833\n",
      "merging in process\n",
      "iteration is  5329\n",
      "CNN_ENG_20030508_170552.18-E10-36\n",
      "it is in  5330\n",
      "merging in process\n",
      "iteration is  5706\n",
      "CNNHL_ENG_20030331_193419.9-E3-2\n",
      "CNNHL_ENG_20030331_193419.9-E3-2\n",
      "in northern iraq u s warplanes hit targets including a ridge east of mosul where iraqi troops have been entrenched\n",
      "error\n",
      "iteration is  5767\n",
      "CNN_ENG_20030422_213527.4-E50-84\n",
      "it is in  5768\n",
      "merging in process\n",
      "iteration is  6063\n",
      "CNN_ENG_20030526_183538.3-E35-97\n",
      "it is in  6064\n",
      "merging in process\n",
      "iteration is  6121\n",
      "CNN_ENG_20030605_085831.13-E37-28\n",
      "it is in  6122\n",
      "merging in process\n",
      "iteration is  6726\n",
      "CNNHL_ENG_20030624_133331.33-E11-49\n",
      "it is in  6727\n",
      "merging in process\n",
      "iteration is  7679\n",
      "CNN_ENG_20030625_210122.0-E6-10\n",
      "it is in  7680\n",
      "merging in process\n",
      "iteration is  7885\n",
      "CNN_ENG_20030624_065843.24-E1-6\n",
      "it is in  7886\n",
      "merging in process\n",
      "iteration is  8201\n",
      "CNN_ENG_20030411_233701.11-E20-40\n",
      "it is in  8202\n",
      "merging in process\n",
      "iteration is  8325\n",
      "CNN_ENG_20030416_180808.15-E28-25\n",
      "it is in  8326\n",
      "merging in process\n",
      "iteration is  8349\n",
      "CNN_ENG_20030624_153103.16-E28-43\n",
      "it is in  8350\n",
      "merging in process\n",
      "iteration is  9553\n",
      "CNN_ENG_20030331_193655.14-E28-79\n",
      "it is in  9554\n",
      "merging in process\n",
      "iteration is  9569\n",
      "CNN_ENG_20030526_183538.3-E10-17\n",
      "it is in  9570\n",
      "merging in process\n",
      "iteration is  9643\n",
      "CNN_ENG_20030605_085831.13-E39-26\n",
      "it is in  9644\n",
      "merging in process\n",
      "iteration is  10238\n",
      "CNNHL_ENG_20030624_133331.33-E8-52\n",
      "it is in  10239\n",
      "merging in process\n",
      "iteration is  11150\n",
      "CNN_ENG_20030612_072835.2-E14-18\n",
      "it is in  11151\n",
      "merging in process\n",
      "iteration is  11400\n",
      "CNN_ENG_20030624_065843.24-E6-10\n",
      "it is in  11401\n",
      "merging in process\n",
      "iteration is  11458\n",
      "CNN_ENG_20030424_183556.7-E21-58\n",
      "it is in  11459\n",
      "merging in process\n",
      "iteration is  11546\n",
      "CNN_ENG_20030430_093016.0-E50-100\n",
      "it is in  11547\n",
      "merging in process\n",
      "iteration is  11575\n",
      "CNN_ENG_20030624_082841.12-E10-19\n",
      "it is in  11576\n",
      "merging in process\n",
      "iteration is  11716\n",
      "CNN_ENG_20030411_233701.11-E20-43\n",
      "it is in  11717\n",
      "merging in process\n",
      "iteration is  11908\n",
      "APW_ENG_20030408.0090-E13-25\n",
      "it is in  11909\n",
      "merging in process\n",
      "iteration is  12136\n",
      "NYT_ENG_20030630.0079-E10-124\n",
      "it is in  12137\n",
      "merging in process\n",
      "iteration is  12186\n",
      "APW_ENG_20030325.0786-E22-58\n",
      "it is in  12187\n",
      "merging in process\n",
      "iteration is  12486\n",
      "APW_ENG_20030403.0862-E32-48\n",
      "APW_ENG_20030403.0862-E32-48\n",
      "capture of the airport would give american and british troops a facility for airlifting equipment and troops to baghdad\n",
      "error\n",
      "iteration is  13039\n",
      "AFP_ENG_20030327.0224-E50-68\n",
      "it is in  13040\n",
      "merging in process\n",
      "iteration is  13660\n",
      "APW_ENG_20030326.0190-E13-18\n",
      "APW_ENG_20030326.0190-E13-18\n",
      "as the trucks lumbered past blasted buildings on the iraq kuwait border an iraqi boy of about 10 pointed to his mouth and shouted eat eat\n",
      "error\n",
      "iteration is  14123\n",
      "APW_ENG_20030325.0786-E24-60\n",
      "it is in  14124\n",
      "merging in process\n",
      "iteration is  14744\n",
      "APW_ENG_20030311.0775-E73-178\n",
      "it is in  14745\n",
      "merging in process\n",
      "iteration is  14937\n",
      "AFP_ENG_20030327.0224-E63-104\n",
      "it is in  14938\n",
      "merging in process\n",
      "iteration is  15220\n",
      "AFP_ENG_20030522.0878-T3-1\n",
      "AFP_ENG_20030522.0878-T3-1\n",
      "moscow may 23 afp highlighting close ties between moscow and beijing chinese president hu jintao monday will make russia his first foreign destination since becoming leader in a visit that could seal a major pipeline accord\n",
      "error\n",
      "iteration is  15237\n",
      "AFP_ENG_20030522.0878-E3-113\n",
      "it is in  15238\n",
      "merging in process\n",
      "iteration is  15460\n",
      "APW_ENG_20030619.0383-E29-57\n",
      "APW_ENG_20030619.0383-E29-57\n",
      "naxakis and two of ocalan s kurdish associates are being tried on criminal charges of endangering national security\n",
      "error\n",
      "iteration is  15565\n",
      "APW_ENG_20030319.0545-E30-68\n",
      "it is in  15566\n",
      "merging in process\n",
      "15651\n"
     ]
    }
   ],
   "source": [
    "update_list = []\n",
    "count = 0\n",
    "passing = 0\n",
    "removing = []\n",
    "print(len(data.data))\n",
    "for _iter in range(len(data.data)):\n",
    "    if count < passing:\n",
    "        count += 1\n",
    "        continue\n",
    "    else:\n",
    "        count = 0\n",
    "        passing = 0\n",
    "    if len(data.data[_iter]['all_relations']) != 0:\n",
    "        print(\"iteration is \", _iter)\n",
    "        for relation in data.data[_iter]['all_relations']:\n",
    "            if relation in data.data[_iter]['relations']:\n",
    "                print(\"here it is \")\n",
    "                continue\n",
    "            search = None\n",
    "            check = False\n",
    "            if relation[0] in [phrase['id'] for phrase in data.data[_iter]['phrases']]:\n",
    "                search = relation[1]\n",
    "            else:\n",
    "                search = relation[0]\n",
    "            print(search)\n",
    "            for j in range(1,3):\n",
    "                if search in [phrase['id'] for phrase in data.data[_iter+j]['phrases']]:\n",
    "                    print(\"it is in \", _iter+j)\n",
    "                    check = True\n",
    "                    new_start = len(data.data[_iter]['words'])\n",
    "                    passing = j\n",
    "                    for i in range(1, j+1):\n",
    "                        print(\"merging in process\")\n",
    "                        removing.append(_iter+i)\n",
    "                        data.data[_iter]['words'].extend(data.data[_iter+i]['words'])\n",
    "                        for phrase in data.data[_iter+i]['phrases']:\n",
    "                            phrase['before'] += new_start\n",
    "                        data.data[_iter]['phrases'].extend(data.data[_iter+i]['phrases'])\n",
    "                        data.data[_iter]['relations'].extend(data.data[_iter+i]['relations'])\n",
    "                        data.data[_iter]['all_relations'].extend(data.data[_iter+i]['all_relations'])\n",
    "                        data.data[_iter]['sentence'] = \" \".join([data.data[_iter]['sentence'], data.data[_iter+i]['sentence']])\n",
    "                        new_start += len(data.data[_iter+i]['words'])\n",
    "                    break\n",
    "            if not check:\n",
    "                print(search)\n",
    "#                 print(relation)\n",
    "                print(data.data[_iter]['sentence'])\n",
    "#                 print(data.data[_iter]['relations'])\n",
    "#                 print(data.data[_iter]['all_relations'])\n",
    "                print(\"error\")\n",
    "            break\n",
    "                \n",
    "# for remove in removing:\n",
    "#     del(data.data[remove])\n",
    "print(len(data.data))              \n",
    "            \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for remove in removing:\n",
    "    del(data.data[remove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data.data:\n",
    "    phrase_ids = [phrase['id'] for phrase in item['phrases']]\n",
    "    for relation in item['relations']:\n",
    "        if relation[0] not in phrase_ids or relation[1] not in phrase_ids:\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10361\n",
      "2607\n",
      "2638\n",
      "15606\n",
      "0\n",
      "70005\n",
      "[0.9633815356778724, 0.9895314272584976, 0.8705106480750175, 0.9934014963309735, 0.991685454571564, 0.9944282493502018, 0.9457400519838591, 0.25132113675201406]\n",
      "278548\n"
     ]
    }
   ],
   "source": [
    "train = [item for item in data.data if item['words'][0]['set'] == \"train\"]\n",
    "valid = [item for item in data.data if item['words'][0]['set'] == \"valid\"]\n",
    "test = [item for item in data.data if item['words'][0]['set'] == \"test\"]\n",
    "print(len(train))\n",
    "print(len(valid))\n",
    "print(len(test))\n",
    "print(len(data.data))\n",
    "print(len([item for item in data.data if item['words'][0]['set'] == \"ignore\"]))\n",
    "items = [item[\"words\"] for item in data.data]\n",
    "import itertools\n",
    "merged = list(itertools.chain.from_iterable(items))\n",
    "ratios = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "def index_finder(_class, choices=[\"ORG\", \"FAC\", \"PER\", \"VEH\", \"LOC\", \"WEA\", \"GPE\", \"-O-\"]):\n",
    "        return choices.index(_class)\n",
    "\n",
    "print(len([item for item in merged if item['w-type'] != \"-O-\"]))\n",
    "_sum = 0\n",
    "for item in merged:\n",
    "    ratios[index_finder(item['w-type'])] += 1\n",
    "    _sum += 1\n",
    "for _it in range(len(ratios)):\n",
    "    ratios[_it] = 1- (ratios[_it] / _sum)\n",
    "print(ratios)\n",
    "print(_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "for it in range(int(len(train) / 1000)):\n",
    "    with open('ACE_JSON/train/result'+str(it)+'.json', 'w+') as fp:\n",
    "        json.dump(train[it*1000:(it+1)*1000], fp)\n",
    "if it:\n",
    "    number = it+1\n",
    "else:\n",
    "    number = it\n",
    "with open('ACE_JSON/train/result'+str(number)+'.json', 'w+') as fp:\n",
    "        json.dump(train[(number)*1000:], fp)\n",
    "        \n",
    "it = 0\n",
    "for it in range(int(len(valid) / 1000)):\n",
    "    with open('ACE_JSON/valid/result'+str(it)+'.json', 'w+') as fp:\n",
    "        json.dump(valid[it*1000:(it+1)*1000], fp)\n",
    "if it:\n",
    "    number = it+1\n",
    "else:\n",
    "    number = it\n",
    "with open('ACE_JSON/valid/result'+str(number)+'.json', 'w+') as fp:\n",
    "        json.dump(valid[(number)*1000:], fp)\n",
    "\n",
    "it = 0\n",
    "for it in range(int(len(test) / 1000)):\n",
    "    with open('ACE_JSON/test/result'+str(it)+'.json', 'w+') as fp:\n",
    "        json.dump(test[it*1000:(it+1)*1000], fp)\n",
    "if it:\n",
    "    number = it+1\n",
    "else:\n",
    "    number = it\n",
    "with open('ACE_JSON/test/result'+str(number)+'.json', 'w+') as fp:\n",
    "        json.dump(test[(number)*1000:], fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
